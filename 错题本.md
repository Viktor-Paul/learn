```
1.在jQuery中需要选取<p>元素里所有<a>元素，则下列选择器写法正确的是（）
A.$(“p a”)
B.$(“p~a”)
C.$(“p + a”)
D.$(“p&gt;a”)
【正确答案】A
【答题时间】2019-07-29 08:15:24
【答案解析】在jQuery中想获取元素的话最简单的方式是使用选择器。并且jQuery中的选择器跟css中的选择器是一样的。如果想获取某元素中所有的其他元素如果不考虑层级的话可以使用后代选择器，即答案$(“p a”)的写法，如果只想获取下一级的话则要使用子代选择器即答案$(“p>a”)的写法。+表示的是匹配相邻兄弟的选择器，~表示的是匹配某元素后所有的兄弟元素
```
import sys
sys.executable

```
2.在Django的模型类中现有如下代码：
class Users(models.Model):
   uphone = models.CharField(max_length=30)
   upwd = models.CharField(max_length=30)
   uname = models.CharField(max_length=30)
class Order(models.Model):
  … …
  users = models.ForeignKey(Users)
def order_user_views(request):
  order = Order.objects.get(id=33502)
现想通过order对象获取到其对应的Users对象，以下代码哪个能够实现：
A.users = order.users.all() 
B.users = order.users 
C.users = order.objects.users() 
D.users = order.objects.users.all() 
【正确答案】B
【答题时间】2019-07-29 08:15:24
【答案解析】在Django的ORM中能够完成关系映射，并且也能完成关系查询（一对一，一对多以及多对多）。该题主要就是一对多的体现，其中users是一，order是多，即一个用户可以用多个订单，所以在order的实体中增加了对users的引用。既然是在order中增加了对users的引用，所以在查询的时候就可以通过order.users的方式直接获取到其关联的users是谁，所以正确答案应该选择users = order.users
```

```
如何在新窗口打开超链接( )
A.<a href=”url”new> 
B.<a href=”url”target=”_blank”> 
C.<a href=”url”target=”_self”> 
D.<a href=”url”target=”_parent”> 
【正确答案】B
【答题时间】2019-07-29 08:15:24
【答案解析】超链接默认在当前窗口打开文件,可以使用target属性,取值_blank设置新建窗口打开
```

```
var n = "danei keji".indexOf("nei",6)；n的值为：（    ）
A.报错 
B.-1 
C.9 
D.5 
【正确答案】B
【答题时间】2019-07-29 08:15:24
【答案解析】indexOf（value,fromIndex）方法用于从指定下标位置从前向后查找字符串value，返回找到的下标；查找失败返回-1
```

```
下列哪种方式可以实现指向右侧的黑色三角标
A.border : 10px solid rgba(0,0,0,0); border-left : 10px solid #000; 
B.border : 10px solid rgba(0,0,0,0); border-top : 10px solid #000; 
C.border : 10px solid transparent; border-right : 10px solid #000; 
D.border : 10px solid #000; 
【正确答案】A
【答题时间】2019-07-29 08:15:24
【答案解析】网页中可以通过边框制作网页三角标，实现步骤为：1. 块元素宽高为0；2. 设置四个方向边框为透明色；3. 指定方向边框颜色显示
```

```
当表单各项添写完毕,鼠标单击提出交按钮时可以触发( )事件
A.onenter 
B.onchange 
C.onsubmit 
D.oninput 
【正确答案】C
【答题时间】2019-07-29 08:15:24
【答案解析】表单事件中,当用户点击提交按钮时会自动触发form的onsubmit事件,验证表单数据是否可以提交给服务器
```

```
下面代码的运行结果是：第一次弹（    ）第二次弹（    ）
 function fn1() {
        alert(1);
  }
    alert( fn1() );
A.function fn1( ) { alert(1) ;} 
B.alert(1) 
C.1 
D.undefined 
【正确答案】C,D
【答题时间】2019-07-29 08:15:24
【答案解析】函数fn1没有返回值，alert(fn1())具体执行时需要先执行fn1(),弹框显示1；但是函数fn1没有返回值，所以外层弹框显示undefined
```

```
在Django的表单中提交数据给服务器，如果想解决跨站点攻击的话，可以采用下列的哪几种方式
A.删除settings.py中MIDDLEWARE中的CsrfViewMiddleWare中间件 
B.在视图处理函数上增加@csrf_protect装饰漆 
C.在模板中<form>下增加{%csrf_token%}中间件 
D.将POST提交方式更改为GET提交方式 
【正确答案】A,B,C
【答题时间】2019-07-29 08:15:24
【答案解析】在Django中如果使用post提交方式提交表单数据的话默认会有跨站点攻击的拦截，目的是为了保证服务器的安全，如果想解决跨站点攻击拦截的话一共有三种方式：1、直接删除settings.py中的MIDDLEWARE中的CsrfViewMiddleWare中间件，但不推荐2、在视图处理函数上增加 @csrf_protect 装饰器，也能解决跨站点攻击的拦截，但这种方式也不推荐3、在下的第一行上增加{%csrf_token%}标签，这种是推荐的实现方式，它表示表单在服务器是受信任的
```

```
下列哪个属性不是盒模型的组成部分
A.outline 
B.width 
C.margin 
D.line-height 
【正确答案】A,D
【答题时间】2019-07-29 08:15:24
【答案解析】CSS盒模型由内容尺寸width height，边框border，内边距padding和外边距margin组成
```

```
下列哪些元素没有默认外边距
A.body 
B.input 
C.div 
D.form 
【正确答案】B,C,D
【答题时间】2019-07-29 08:15:24
【答案解析】body带有默认8像素的外边距，其他div form input都没有默认外边距
```

```
下面有关Django中的静态文件说法错误的是
A.在模板中允许过 {%static%}标签来访问呢静态文件 
B.静态文件可以像HTML一样直接通过相对路径，绝对路径和根相对路径来访问 
C.访问静态文件必须要设置静态文件的存储路径和访问路径 
D.需要在settings.py中通过设置STATIC_URL 变量来设置静态文件的存储路径，通过STATICFIELS_DIRS 变量来设置静态文件的访问路径 
正确答案】B,D
【答题时间】2019-07-29 08:15:24
【答案解析】在Django中的静态文件必须要设置访问路径和存储路径后才能获取，不能像html一样直接使用相对路径、绝对路径和根相对路径来访问。访问路径和存储路径是在settings.py中设置的，通过 STATIC_URL 变量来设置静态文件的访问路径，通过STATICFILES_DIRS 来设置静态文件的存储路径。设置好之后在模板中可以通过两种方式来进行访问。第一种方式：通过 {%statitc  ‘url’%} 标签来访问，Django会自己拼接静态文件的访问路径。第二种方式：通过 STATIC_URL 后面的值来拼凑根相对路径的方式来进行访问
```

```
下列有关盒模型属性的描述正确的是
A.margin用于设置元素与元素之间的距离 
B.padding用于设置元素与元素之间的距离 
C.添加border不影响元素在文档中的占据尺寸 
D.添加border会影响元素在文档中的占据尺寸 
【正确答案】A,D
【答题时间】2019-07-29 08:15:24
【答案解析】CSS盒模型相关的属性有： width height margin padding border，任何一个属性都会影响元素在文档中的实际占据尺寸；margin表示元素与元素之间的外边距，padding表示元素内容与边框之间的内边距
```

```
A.box-sizing : border-box；更改元素盒模型的计算方式，width/height尺寸表示的是内容+padding的大小 
B.默认元素在文档中的最终尺寸由widt/height+padding+border+margin计算得到 
C.可以通过box-sizing属性更改元素盒模型的计算方式，默认值为border-box 
D.box-sizing 默认为content-box 
【正确答案】B,D
【答题时间】2019-07-29 08:15:24
【答案解析】box-sizing 用于设置元素最终尺寸的计算方式，默认为content-box,元素设置width/height属性对应内容框，可以修改为border-box，width/height属性对应为边框以内区域的大小（width/height=border+padding+内容）,添加边框或内边距会影响内容框的尺寸
```

爬虫知识

```
(多选题)关于urllib和urllib2的区别说法正确的是（）
A.urllib2可以接受一个Request类的实例来设置URL请求的headers，urllib仅可以接受URL。这意味着，你不可以通过urllib模块伪装你的User Agent字符串等（伪装浏览器） 
B.urllib提供urlencode方法用来GET查询字符串的产生，而urllib2没有。这是为何urllib常和urllib2一起使用的原因。 
C.urllib2模块比较优势的地方是urlliburllib2.urlopen可以接受Request对象作为参数，从而可以控制HTTP Request的header部。 
D.但是urllib.urlretrieve函数以及urllib.quote等一系列quote和unquote功能没有被加入urllib2中，因此有时也需要urllib的辅助。 
urllib：优势
urllib支持设置编码的函数，urllib.urlencode,在模拟登陆的时候，经常要post编码之后的参数，所以要想不使用第三方库完成模拟登录，你就需要使用urllib。
urllib2:优势
urllib2可以用urllib2.openurl中设置Request参数，来修改Header头
【正确答案】A,B,C,D
【答案解析】以上说法均为正确选项
```

```
(多选题)关于urllib库说法正确的是（）
A.urllib.request 请求模块
B.urllib.error	异常处理模块 
C.urllib.parse	url解析模块 
D.urllib.robotparser	robots.txt解析模块 
【正确答案】A,B,C,D
【答案解析】以上说法均为正确选项
```

```
http和https
(多选题)关于http和https说法正确的是（）
A.https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用 
B.http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议 
C.http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443 
D.http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 
【正确答案】A,B,C,D
【答案解析】以上均为正确选项
2(多选题)下列关于http和https概念说法正确的是（）
A.HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。 
B.HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 
C.HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 
D.以上说法都不对 
【正确答案】A,B,C
【答案解析】ABC为正确选项，为基本概念
```

SSL

```
SSL(Secure Sockets Layer 安全套接层),及其继任者传输层安全（Transport Layer Security，TLS）是为网络通信提供安全及数据完整性的一种安全协议。TLS与SSL在传输层对网络连接进行加密。

解析：
SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。

提供服务
1）认证用户和服务器，确保数据发送到正确的客户机和服务器；
2）加密数据以防止数据中途被窃取；
3）维护数据的完整性，确保数据在传输过程中不被改变。
```

正则

```
1(单选题)张三zs333@domain.com 下列哪个正则能匹配到（）
A.[A-Za-z0-9\u4e00-\u9fa5\W]+ 
B.^\\{2}[\w-]+\\(([\w-][\w-\s]*[\w-]+[$$]?$)|([\w-][$$]?$)) 
C.^(\\+\\d{2}-)?0\\d{2,3}-\\d{7,8}$ 
D.以上说法都不对 
【正确答案】A
【答案解析】汉字在正则中表示为[\u4e00-\u9fa5]
字母和数字表示为A-Za-z0-9
C选项是匹配固定电话
2.
(多选题)下列关于正则re中方法正确的是（）
A.re.split(pattern, string[, maxsplit])
按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割 
B.re.findall(pattern, string[, flags]) 
搜索string，以列表形式返回全部能匹配的子串 
C.re.finditer(pattern, string[, flags])
搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器 
D.re.sub(pattern, repl, string[, count])
使用repl替换string中每一个匹配的子串后返回替换后的字符串。 
【正确答案】A,B,C,D
【答案解析】以上说法均为正确选项，并且为清洗数据常用方法
```

算法

```
(单选题)1-9这9个数字中，选3个出来，其和为奇数的组合有几个？
A.30 
B.40 
C.50 
D.60 
【正确答案】B
【答案解析】一种奇 奇 奇，5选3，10种，c(3,5)。
一种奇 偶 偶，5选1 * 4选2，30种，c(1,5) * c(2,4)。
```

双向队列

```

(单选题)关于python双向队列提供的方法，说法错误的是（）
A.appendleft(x) ：在双向队列的左边追加x 
B.extend(iterable) ：拓展双向队列的右侧通过迭代iterable中的每个元素，依次追加在右侧 
C.popleft() ：从双向队列的左侧删除并返回一个元素，如果双向队列目前没有元素，会引发一个IndexError 
D.pop() ：从双向队列的左侧删除并返回一个元素，如果双向队列目前没有元素，会引发一个IndexError 
【正确答案】D
【答案解析】pop()：从双向队列的右侧删除并返回一个元素，如果双向队列目前没有元素，会引发一个IndexError
2
、
(单选题)关于下列双向队列deque的说法，错误的是
A.deque 可以迭代 
B.deque 可以使用函数len求长度 
C.deque 可以用 in 操作符判断是否含有某一元素 
D.deque 尾部插入或删除元素，时间复杂度是O（1），但是从 deque 头部插入或移除元素，时间复杂度就是O（n） 
【正确答案】D
【答案解析】deque 头部或尾部插入或移除一个元素，时间复杂度为O（1），就是不会因数据的大小及位置等因素而变化。
```

进程池

```
关于进程池Pool的方法，说法错误的是（）
A.apply_async(func[, args=()[, kwds={}[, callback=None]]])：该函数用于传递不定参数，主进程不会被阻塞且支持结果返回进行回调。 
B.map(func, iterable[, chunksize=None])：Pool类中的map方法，与内置的map函数用法行为基本一致，它会使进程阻塞直到返回结果。 
注意，虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程。 
C.join()：主进程阻塞等待子进程的退出，join方法必须在close或terminate之后使用。 
D.close()：结束工作进程，不在处理未处理的任务。 
【正确答案】D
【答案解析】close()：关闭进程池（pool），使其不在接受新的任务。
terminate()：结束工作进程，不在处理未处理的任务。
多选题 （共计 1 题，总计 2分）
2
(多选题)关于Pool进程池，说法正确的有（）
A.Pool可以提供指定数量的进程供用户调用 
B.当有新的请求提交到pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求 
C.但如果池中的进程数已经达到规定最大值，那么该请求就会等待，直到池中有进程结束，才会创建新的进程来它 
D.利用multiprocessing下的Pool可以很方便的同时自动处理几百或者上千个并行操作，脚本的复杂性也大大降低。 
【正确答案】A,B,C,D
【答案解析】ABCD都对
```

Scrapy

```
(多选题)以下是item pipeline的一些典型应用有（）
A.清理HTML数据 
B.验证爬取的数据(检查item包含某些字段) 
C.查重(并丢弃) 
D.将爬取结果保存到数据库中 
【正确答案】A,B,C,D
【答案解析】ABCD都对

Rule的LinkExtractor类的主要参数有：
A.allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 
B.deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。 
C.restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。 
D.allow_domains：一定不会被提取链接的domains。 
【正确答案】A,B,C
【答案解析】allow_domains：会被提取的链接的domains。
deny_domains：一定不会被提取链接的domains。
参考：
https://blog.csdn.net/wqh_jingsong/article/details/56865433
```

```
class scrapy.spiders.CrawlSpider
This is the most commonly used spider for crawling regular websites, as it provides a convenient mechanism for following links by defining a set of rules. It may not be the best suited for your particular web sites or project, but it’s generic enough for several cases, so you can start from it and override it as needed for more custom functionality, or just implement your own spider.

Apart from the attributes inherited from Spider (that you must specify), this class supports a new attribute:

rules
Which is a list of one (or more) Rule objects. Each Rule defines a certain behaviour for crawling the site. Rules objects are described below. If multiple rules match the same link, the first one will be used, according to the order they’re defined in this attribute.

This spider also exposes an overrideable method:

parse_start_url(response)
This method is called for the start_urls responses. It allows to parse the initial responses and must return either an Item object, a Request object, or an iterable containing any of them.

Crawling rules
class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
link_extractor is a Link Extractor object which defines how links will be extracted from each crawled page. Each produced link will be used to generate a Request object, which will contain the link’s text in its meta dictionary (under the link_text key).

callback is a callable or a string (in which case a method from the spider object with that name will be used) to be called for each link extracted with the specified link extractor. This callback receives a Response as its first argument and must return either a single instance or an iterable of Item, dict and/or Request objects (or any subclass of them). As mentioned above, the received Response object will contain the text of the link that produced the Request in its meta dictionary (under the link_text key)

Warning

When writing crawl spider rules, avoid using parse as callback, since the CrawlSpider uses the parse method itself to implement its logic. So if you override the parse method, the crawl spider will no longer work.

cb_kwargs is a dict containing the keyword arguments to be passed to the callback function.

follow is a boolean which specifies if links should be followed from each response extracted with this rule. If callback is None follow defaults to True, otherwise it defaults to False.

process_links is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified link_extractor. This is mainly used for filtering purposes.

process_request is a callable (or a string, in which case a method from the spider object with that name will be used) which will be called for every Request extracted by this rule. This callable should take said request as first argument and the Response from which the request originated as second argument. It must return a Request object or None (to filter out the request).

CrawlSpider example
Let’s now take a look at an example CrawlSpider with rules:

import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = (
        # Extract links matching 'category.php' (but not matching 'subsection.php')
        # and follow links from them (since no callback means follow=True by default).
        Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

    def parse_item(self, response):
        self.logger.info('Hi, this is an item page! %s', response.url)
        item = scrapy.Item()
        item['id'] = response.xpath('//td[@id="item_id"]/text()').re(r'ID: (\d+)')
        item['name'] = response.xpath('//td[@id="item_name"]/text()').get()
        item['description'] = response.xpath('//td[@id="item_description"]/text()').get()
        item['link_text'] = response.meta['link_text']
        return item
This spider would start crawling example.com’s home page, collecting category links, and item links, parsing the latter with the parse_item method. For each item response, some data will be extracted from the HTML using XPath, and an Item will be filled with it.

CrawlSpider是爬取那些具有一定规则网站的常用的爬虫，它基于Spider并有一些独特属性

rules: 是Rule对象的集合，用于匹配目标网站并排除干扰 
parse_start_url: 用于爬取起始响应，必须要返回Item，Request中的一个。 
因为rules是Rule对象的集合，所以这里也要介绍一下Rule。它有几个参数：link_extractor、callback=None、cb_kwargs=None、follow=None、process_links=None、process_request=None 
其中的link_extractor既可以自己定义，也可以使用已有LinkExtractor类，主要参数为：

allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。 
deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。 
allow_domains：会被提取的链接的domains。 
deny_domains：一定不会被提取链接的domains。 
restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。还有一个类似的restrict_css 
```

